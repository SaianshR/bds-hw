{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downloading all papers\n",
    "import requests\n",
    "import re\n",
    "\n",
    "volume = 80\n",
    "cont = requests.get('http://proceedings.mlr.press/v' + str(volume) + '/').text\n",
    "\n",
    "paper_list = re.findall(\n",
    "    r'(http://proceedings\\.mlr\\.press/v' + str(volume) + r'/.*?\\.pdf)', cont)\n",
    "paperlist = list(set(paper_list))\n",
    "\n",
    "#for item in paperlist:\n",
    "#    cont = requests.get(item).content\n",
    "#    print('Download ' + item)\n",
    "#    f = open(re.findall(r'http://proceedings\\.mlr\\.press/v' +\n",
    "#                        str(volume) + r'/.*?/(.*?\\.pdf)', item)[0], 'wb+')\n",
    "#    f.write(cont)\n",
    "#    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Superfluous whitespace found in object header b'1' b'0'\n",
      "Superfluous whitespace found in object header b'2' b'0'\n",
      "Superfluous whitespace found in object header b'3' b'0'\n",
      "Superfluous whitespace found in object header b'16' b'0'\n",
      "Superfluous whitespace found in object header b'15' b'0'\n",
      "Superfluous whitespace found in object header b'4' b'0'\n",
      "Superfluous whitespace found in object header b'5' b'0'\n",
      "Superfluous whitespace found in object header b'6' b'0'\n",
      "Superfluous whitespace found in object header b'7' b'0'\n",
      "Superfluous whitespace found in object header b'8' b'0'\n",
      "Superfluous whitespace found in object header b'9' b'0'\n",
      "Superfluous whitespace found in object header b'10' b'0'\n",
      "Superfluous whitespace found in object header b'11' b'0'\n",
      "Superfluous whitespace found in object header b'12' b'0'\n",
      "Superfluous whitespace found in object header b'13' b'0'\n",
      "Superfluous whitespace found in object header b'14' b'0'\n",
      "Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      "Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      "Superfluous whitespace found in object header b'1' b'0'\n",
      "Superfluous whitespace found in object header b'2' b'0'\n",
      "Superfluous whitespace found in object header b'3' b'0'\n",
      "Superfluous whitespace found in object header b'14' b'0'\n",
      "Superfluous whitespace found in object header b'29' b'0'\n",
      "Superfluous whitespace found in object header b'40' b'0'\n",
      "Superfluous whitespace found in object header b'57' b'0'\n",
      "Superfluous whitespace found in object header b'75' b'0'\n",
      "Superfluous whitespace found in object header b'91' b'0'\n",
      "Superfluous whitespace found in object header b'107' b'0'\n",
      "Superfluous whitespace found in object header b'125' b'0'\n",
      "Superfluous whitespace found in object header b'140' b'0'\n",
      "Superfluous whitespace found in object header b'13' b'0'\n",
      "Superfluous whitespace found in object header b'4' b'0'\n",
      "Superfluous whitespace found in object header b'5' b'0'\n",
      "Superfluous whitespace found in object header b'6' b'0'\n",
      "Superfluous whitespace found in object header b'7' b'0'\n",
      "Superfluous whitespace found in object header b'8' b'0'\n",
      "Superfluous whitespace found in object header b'9' b'0'\n",
      "Superfluous whitespace found in object header b'10' b'0'\n",
      "Superfluous whitespace found in object header b'11' b'0'\n",
      "Superfluous whitespace found in object header b'12' b'0'\n",
      "Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      "Xref table not zero-indexed. ID numbers for objects will be corrected.\n",
      "Xref table not zero-indexed. ID numbers for objects will be corrected.\n"
     ]
    }
   ],
   "source": [
    "# put all text into single .txt file\n",
    "import io, requests, string\n",
    "from PyPDF2 import PdfFileReader\n",
    "\n",
    "dump = open(\"dump.txt\", \"w\")\n",
    "\n",
    "def punctuation_extermination(s):\n",
    "    o = str.maketrans('', '', string.punctuation)\n",
    "    return str(s).translate(o)\n",
    "\n",
    "\n",
    "for url in paperlist:\n",
    "    r = requests.get(url)\n",
    "    f = io.BytesIO(r.content)\n",
    "    reader = PdfFileReader(f)\n",
    "    contents = reader.getPage(0).extractText().split('\\n')\n",
    "    contentstring = str(contents)\n",
    "    to_write = punctuation_extermination(contentstring)\n",
    "    dump.write(to_write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top 10 most frequent words (w/o stopwords)\n",
    "from nltk import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "corpus = []\n",
    "with open('dump.txt','r') as file:\n",
    "    corpus = [word for line in file for word in line.split()]\n",
    "\n",
    "corpus_without_stop = [word for word in corpus if word.lower() not in stopwords.words(\"english\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['problem',\n",
       " 'function',\n",
       " 'algorithm',\n",
       " 'training',\n",
       " 'methods',\n",
       " 'set',\n",
       " 'gradient',\n",
       " 'network',\n",
       " 'number',\n",
       " 'distribution']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "banned = ['et', 'al', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '2015', '2016', '2017', '2018', 'learning', 'models', 'algorithms', 'using']\n",
    "# banning common sense / meaningless / plural words\n",
    "corpus_without_stop = [word for word in corpus_without_stop if word.lower() not in banned]\n",
    "freqdistr = FreqDist(corpus_without_stop)\n",
    "top10words = [element[0] for element in freqdistr.most_common(10)]\n",
    "\n",
    "top10words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2618514677902033"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# entropy\n",
    "import math\n",
    "\n",
    "megastring = ' '.join(corpus)\n",
    "wrd = \"regression\" # random word\n",
    "count_of_word = megastring.count(wrd)\n",
    "total_length = len(set(megastring))\n",
    "prob = count_of_word/total_length\n",
    "log_prob = math.log(prob, 2)\n",
    "entropy = prob * log_prob\n",
    "entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AlvarezHamelin considered usually 2Michael Correspondence visualizing Adversarial reinforcement explicitly wl1vecx00 decomposition badly regret randomness depth Nash Beck thus athttpssitesgooglecomview 2007 R principle language iterative University methods problem comes study Abstract bias show even walk many Large allevi Castro guarantees Mnih write assessment obtain need utilization x0 solvers x151x00q solutions label object items Line deﬁne G4 us dataset allow spaces recently bestknown Sys cheaper constant make perturbed lognslashleftc2for v2Cjrvhjkvx00rhk2 Science samples fuv2Eis LM performances 3Varun Rx unknown separate Physics instances tuned dimension x12and recent distributed variables NNs ap Mnih Introduction expected ysuicaltechedu Correspondence numbers even Ixy China3Department OPE underlying regret networks'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generating paragraph using marginal distr\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df['words'] = [element[0] for element in freqdistr.most_common(1000000000000000)]\n",
    "df['frequencies'] = [element[1] for element in freqdistr.most_common(1000000000000000)]\n",
    "df['probabilities'] = df['frequencies']/len(corpus)\n",
    "df['probabilities'] /= df['probabilities'].sum()\n",
    "df['log_probabilities'] = np.log2(df['probabilities'])\n",
    "\n",
    "p = np.random.choice(df['words'], 100, p=df['probabilities'])\n",
    "p_string = ' '.join(x for x in p)\n",
    "\n",
    "p_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate paragraph using n-gram\n",
    "import string, random\n",
    "from typing import List\n",
    "\n",
    "def tokenize(text: str) -> List[str]:\n",
    "    # tokenizer\n",
    "    for punct in string.punctuation:\n",
    "        text = text.replace(punct, ' '+punct+' ')\n",
    "    t = text.split()\n",
    "    return t\n",
    "\n",
    "def get_ngrams(n: int, tokens: list) -> list:\n",
    "    # make n-rams\n",
    "    # tokens.append('<END>')\n",
    "    tokens = (n-1)*['<START>']+tokens\n",
    "    l = [(tuple([tokens[i-p-1] for p in reversed(range(n-1))]), tokens[i]) for i in range(n-1, len(tokens))]\n",
    "    return l\n",
    "\n",
    "class NgramModel(object):\n",
    "\n",
    "    def __init__(self, n):\n",
    "        self.n = n\n",
    "\n",
    "        # dictionary that keeps list of candidate words given context\n",
    "        self.context = {}\n",
    "\n",
    "        # keeps track of how many times ngram has appeared in the text before\n",
    "        self.ngram_counter = {}\n",
    "\n",
    "    def update(self, sentence: str) -> None:\n",
    "        # update lang model\n",
    "        n = self.n\n",
    "        ngrams = get_ngrams(n, tokenize(sentence))\n",
    "        for ngram in ngrams:\n",
    "            if ngram in self.ngram_counter:\n",
    "                self.ngram_counter[ngram] += 1.0\n",
    "            else:\n",
    "                self.ngram_counter[ngram] = 1.0\n",
    "\n",
    "            prev_words, target_word = ngram\n",
    "            if prev_words in self.context:\n",
    "                self.context[prev_words].append(target_word)\n",
    "            else:\n",
    "                self.context[prev_words] = [target_word]\n",
    "\n",
    "    def prob(self, context, token):\n",
    "        # conditional probability of word\n",
    "        try:\n",
    "            count_of_token = self.ngram_counter[(context, token)]\n",
    "            count_of_context = float(len(self.context[context]))\n",
    "            result = count_of_token / count_of_context\n",
    "\n",
    "        except KeyError:\n",
    "            result = 0.0\n",
    "        return result\n",
    "\n",
    "    def random_token(self, context):\n",
    "        # randomly select next word\n",
    "        r = random.random()\n",
    "        map_to_probs = {}\n",
    "        token_of_interest = self.context[context]\n",
    "        for token in token_of_interest:\n",
    "            map_to_probs[token] = self.prob(context, token)\n",
    "\n",
    "        summ = 0\n",
    "        for token in sorted(map_to_probs):\n",
    "            summ += map_to_probs[token]\n",
    "            if summ > r:\n",
    "                return token\n",
    "\n",
    "    def generate_text(self, token_count: int):\n",
    "        # generator - set no. of words\n",
    "        n = self.n\n",
    "        context_queue = (n - 1) * ['<START>']\n",
    "        result = []\n",
    "        for _ in range(token_count):\n",
    "            obj = self.random_token(tuple(context_queue))\n",
    "            result.append(obj)\n",
    "            if n > 1:\n",
    "                context_queue.pop(0)\n",
    "                if obj == '.':\n",
    "                    context_queue = (n - 1) * ['<START>']\n",
    "                else:\n",
    "                    context_queue.append(obj)\n",
    "        return ' '.join(result)\n",
    "\n",
    "\n",
    "def create_ngram_model(n, path):\n",
    "    m = NgramModel(n)\n",
    "    with open(path, 'r') as f:\n",
    "        text = f.read()\n",
    "        text = text.split('.')\n",
    "        for sentence in text:\n",
    "            # add back the fullstop\n",
    "            sentence += '.'\n",
    "            m.update(sentence)\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoupling GradientLike Learning Rules from Representations Philip S Thomas1Christoph Dann2Emma Brunskill3 Abstract In machine learning learning often corresponds to changing the parameters of a parameterized function A learning rule is an algorithm or math ematical expression that speciﬁes precisely how the parameters should be changed When creating a machine learning system we must make two decisions what representation should be used ie what parameterized function should be used and what learning rule should be used to search through the resulting set of representable func tions In this paper we focus on gradientlike learning rules wherein these two decisions are coupled\n"
     ]
    }
   ],
   "source": [
    "m = create_ngram_model(10, 'dump.txt')\n",
    "# random.seed(2)\n",
    "print(m.generate_text(100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a2f36d4f0fab32c067f747ee04c69255073dcd12dd897c9c524bc34636b9f1b8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
